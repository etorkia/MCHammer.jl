var documenterSearchIndex = {"docs":
[{"location":"manual/LearningCurves/#Learning-Curve-Modeling-Approaches","page":"Learning Curves Models","title":"Learning Curve Modeling Approaches","text":"","category":"section"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"using MCHammer, DataFrames, Distributions, Random, Plots","category":"page"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"Learning curves are mathematical models predicting improvements in productivity and efficiency as experience with a task increases. These curves are essential tools for:","category":"page"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"Estimating project costs and timelines.\nAnalyzing historical data for efficiency trends.\nForecasting and decision-making.","category":"page"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"The following documentation covers three popular methods implemented using Julia with multiple dispatch:","category":"page"},{"location":"manual/LearningCurves/#Learning-Curve-Methods","page":"Learning Curves Models","title":"Learning Curve Methods","text":"","category":"section"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"abstract type LearningCurveMethod end\n    struct WrightMethod <: LearningCurveMethod end\n    struct CrawfordMethod <: LearningCurveMethod end\n    struct ExperienceMethod <: LearningCurveMethod end","category":"page"},{"location":"manual/LearningCurves/#Wright's-Curve","page":"Learning Curves Models","title":"Wright's Curve","text":"","category":"section"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"WrightMethod ","category":"page"},{"location":"manual/LearningCurves/#MCHammer.WrightMethod","page":"Learning Curves Models","title":"MCHammer.WrightMethod","text":"Wright learning curve method. \n\nstruct WrightMethod <: LearningCurveMethod end\n\nIntroduced by T.P. Wright in 1936 in his seminal work on airplane production cost  analysis (Wright, 1936).  This model observes that with each doubling of cumulative production, the unit cost  decreases by a fixed percentage. It is well‐suited for processes where learning is continuous and  gradual.  \n\ntextCost = textInitialEffort times textTotalUnits^fraclog(textLearning)log(2) + 1\n\nWhen to Use:  \n\nWhen historical data show a smooth, predictable decline in unit costs as production doubles.  \n\nWhen to Avoid:  \n\nWhen cost reductions occur in discrete steps or when the production process experiences structural changes.\n\n\n\n\n\n","category":"type"},{"location":"manual/LearningCurves/#Crawford's-Curve","page":"Learning Curves Models","title":"Crawford's Curve","text":"","category":"section"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"CrawfordMethod ","category":"page"},{"location":"manual/LearningCurves/#MCHammer.CrawfordMethod","page":"Learning Curves Models","title":"MCHammer.CrawfordMethod","text":"Crawford learning curve method.\n\nstruct CrawfordMethod <: LearningCurveMethod end\n\nDerived from discrete cumulative cost analysis methods found in operations research, the Crawford  learning curve (e.g., Crawford, 1982) aggregates individual unit costs—which decrease according to a  power‐law function of the unit index—to yield a total cost. Unlike the smooth curves of Wright and Experience, the Crawford method sums unit‐by‐unit  costs that are reduced based on their position in the production sequence.  \n\ntextCost = sum_i=1^textTotalUnits textInitialEffort times i^fraclog(textLearning)log(2)\n\nWhen to Use:  \n\nWhen you have detailed unit cost data and need a granular, discrete analysis of learning effects.  \n\nWhen to Avoid:  \n\nWhen the overall cost trend is continuous and best represented by a smooth curve, in which case  Wright’s or the Experience model might be preferable.\n\n\n\n\n\n","category":"type"},{"location":"manual/LearningCurves/#Experience-Curve","page":"Learning Curves Models","title":"Experience Curve","text":"","category":"section"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"ExperienceMethod","category":"page"},{"location":"manual/LearningCurves/#MCHammer.ExperienceMethod","page":"Learning Curves Models","title":"MCHammer.ExperienceMethod","text":"Experience learning curve method.\n\n    struct ExperienceMethod <: LearningCurveMethod end\n\nPopularized by Bruce Henderson of the Boston Consulting Group in the 1970s, the experience  curve expands on Wright’s observation to encompass total cost reductions (including overhead and other  factors) as cumulative production increases. It suggests that as cumulative output doubles, total costs fall by a constant percentage, reflecting economies of scale and learning effects throughout an organization.  \n\ntextCost = textInitialEffort times (textTotalUnits^textLearning)\n\nWhen to Use:  \n\nFor strategic analysis and competitive planning when broad organizational efficiency improvements  are observed.  \nWhen to Avoid:  \nWhen cost behavior is highly non-linear or when improvements are due to sudden technological breakthroughs.\n\n\n\n\n\n","category":"type"},{"location":"manual/LearningCurves/#Cumulative-Cost-Analysis","page":"Learning Curves Models","title":"Cumulative Cost Analysis","text":"","category":"section"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"To computes cumulative cost analytically for an experience curve, here are the functions to accomplish this.  ","category":"page"},{"location":"manual/LearningCurves/#Wright-Learning-Curve","page":"Learning Curves Models","title":"Wright Learning Curve","text":"","category":"section"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"result = lc_analytic(WrightMethod(), 200, 500, 0.85)\nprintln(result)","category":"page"},{"location":"manual/LearningCurves/#Crawford-Learning-Curve","page":"Learning Curves Models","title":"Crawford Learning Curve","text":"","category":"section"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"result = lc_analytic(CrawfordMethod(), 150, 400, 0.75)","category":"page"},{"location":"manual/LearningCurves/#Experience-Curve-2","page":"Learning Curves Models","title":"Experience Curve","text":"","category":"section"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"result = lc_analytic(ExperienceMethod(), 100, 1000, 0.8)","category":"page"},{"location":"manual/LearningCurves/#Curve-Functions","page":"Learning Curves Models","title":"Curve Functions","text":"","category":"section"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"Generates detailed DataFrame including cumulative, incremental, and average costs.","category":"page"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"lc_curve","category":"page"},{"location":"manual/LearningCurves/#MCHammer.lc_curve","page":"Learning Curves Models","title":"MCHammer.lc_curve","text":"Generate a learning curve as a DataFrame for a given method. \n\nlc_curve(method::LearningCurveMethod, InitialEffort, TotalUnits, Learning; LotSize=1)\n\nThe DataFrame columns are:\n\nUnits: Production unit number.\nCurvePoint: Cumulative cost/effort at that unit.\nIncrementalCost: Difference in cumulative cost from the previous step.\nAvgCost: Average cost per unit up to that point.\nMethod: A string identifier for the method.\n\n\n\n\n\n","category":"function"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"df = lc_curve(WrightMethod(), 200, 500, 0.85; LotSize=25)\nprintln(first(df, 5))","category":"page"},{"location":"manual/LearningCurves/#Analysis-Functions","page":"Learning Curves Models","title":"Analysis Functions","text":"","category":"section"},{"location":"manual/LearningCurves/#Fitting-Functions","page":"Learning Curves Models","title":"Fitting Functions","text":"","category":"section"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"lc_fit","category":"page"},{"location":"manual/LearningCurves/#MCHammer.lc_fit","page":"Learning Curves Models","title":"MCHammer.lc_fit","text":"lc_fit(::ExperienceMethod, InitialEffort, Units; EstLC=0.8)\n\nFit the learning rate for the Experience method by adjusting the rate  until its analytic cumulative cost converges to that computed using Wright's model.\n\n\n\n\n\nlc_fit(::CrawfordMethod, InitialEffort, Units; EstLC=0.8)\n\nFit the learning rate for the Crawford method by adjusting the rate  until its analytic cumulative cost converges to that computed using Wright's model.\n\n\n\n\n\nlc_fit(::WrightMethod, InitialEffort, Units; EstLC=0.8)\n\nWright's method is taken as the baseline model so no fitting is performed.  Returns the provided estimated learning rate.\n\n\n\n\n\n","category":"function"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"Example:","category":"page"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"lc_fit(::ExperienceMethod, InitialEffort, Units; EstLC=0.8)\nlc_fit(::CrawfordMethod, InitialEffort, Units; EstLC=0.8)\nlc_fit(::WrightMethod, InitialEffort, Units; EstLC=0.8)","category":"page"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"best_fit = lc_fit(CrawfordMethod(), 150, 400; EstLC=0.75)","category":"page"},{"location":"manual/LearningCurves/#Learning-Rate-Estimation","page":"Learning Curves Models","title":"Learning Rate Estimation","text":"","category":"section"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"Estimates learning rates using Wright's method from two data points.","category":"page"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"learn_rate","category":"page"},{"location":"manual/LearningCurves/#MCHammer.learn_rate","page":"Learning Curves Models","title":"MCHammer.learn_rate","text":"Estimate the learning rate using Wright's method from two production data points. \n\nlearn_rate(::WrightMethod, TotalUnitsA, WorkUnitA, TotalUnitsB, WorkUnitB)\n\nThis method can be used as a starting point for the Crawford and Experience curves because no closed form solutions exist for these methods\n\n\n\n\n\n","category":"function"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"rate = learn_rate(WrightMethod(), 1, 2000, 144, 8000)\nprintln(rate)","category":"page"},{"location":"manual/LearningCurves/#Comparison-Utility","page":"Learning Curves Models","title":"Comparison Utility","text":"","category":"section"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"Compares learning curves across a range of learning rates.","category":"page"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"learn_rates","category":"page"},{"location":"manual/LearningCurves/#MCHammer.learn_rates","page":"Learning Curves Models","title":"MCHammer.learn_rates","text":"Generate a comparison of cumulative costs for a range of learning rates (0 to 1)  across the three methods: Wright, Crawford, and Experience.\n\nlearn_rates(InitialEffort, Units; LC_Step=0.1)\n\nReturns a DataFrame with columns:\n\nLC: The learning rate.\nWright: Cumulative cost from Wright's model.\nCrawford: Cumulative cost from Crawford's model.\nExperience: Cumulative cost from the Experience model.\n\n\n\n\n\n","category":"function"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"Example:","category":"page"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"rates_df = learn_rates(100, 500; LC_Step=0.05)\nprintln(first(rates_df, 5))","category":"page"},{"location":"manual/LearningCurves/#Picking-the-right-curve","page":"Learning Curves Models","title":"Picking the right curve","text":"","category":"section"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"Sometimes picking the righ curve is challenging and in these cases plotting a comparison of average costs across methods using Plots.jl can be very helpful.","category":"page"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"using Plots\n\nLearnRate = 0.78\nInitialEffort = 50\nUnits = 1000\n\n\nCC = lc_curve(CrawfordMethod(), InitialEffort, Units, LearnRate; LotSize=25)\nWC = lc_curve(WrightMethod(), InitialEffort, Units, LearnRate; LotSize=25)\nEC = lc_curve(ExperienceMethod(), InitialEffort, Units, LearnRate; LotSize=25)\nGraphResults = vcat(CC, WC, EC)\n\nplot(GraphResults.Units, GraphResults.AvgCost, group=GraphResults.Method,\n    xlabel=\"Units\", ylabel=\"Average Cost\", title=\"Average Cost vs Units by Method\",\n    lw=2, legend=:topright)","category":"page"},{"location":"manual/LearningCurves/#Sources-and-References","page":"Learning Curves Models","title":"Sources & References","text":"","category":"section"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"Eric Torkia, Decision Superhero Vol. 2, chapter 6 : SuperPower: The Laws of Nature that Predict, Technics Publishing, 2025\nAvailable on Amazon : https://a.co/d/4YlJFzY . Volumes 2 and 3 to be released in Spring and Fall 2025.\nWright, T.P. (1936). Factors Affecting the Cost of Airplanes. Journal of the Aeronautical Sciences, 3(4), 122–128.\nHenderson, B.D. (1973). Industrial Experience, Technology Transfer, and Cost Behavior. Harvard Business School Working Paper.\nCrawford, D. (1982). Learning Curves: Theory and Practice. Journal of Cost Analysis. ","category":"page"},{"location":"manual/LearningCurves/","page":"Learning Curves Models","title":"Learning Curves Models","text":"\"\"\"","category":"page"},{"location":"tutorials/1_first_model/#Building-your-first-model","page":"My First Model","title":"Building your first model","text":"","category":"section"},{"location":"tutorials/1_first_model/#Installing-MCHammer","page":"My First Model","title":"Installing MCHammer","text":"","category":"section"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"Install the package as usual using Pkg.","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"    using Pkg\n    Pkg.(\"MCHammer\")","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"If you need to install direct, we recommend using ']' to go in the native Pkg manager.","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"    (v1.1) pkg> add https://github.com/etorkia/MCHammer.jl","category":"page"},{"location":"tutorials/1_first_model/#Loading-MCHammer","page":"My First Model","title":"Loading MCHammer","text":"","category":"section"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"To load the MCHammer package","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"using MCHammer","category":"page"},{"location":"tutorials/1_first_model/#Getting-your-environment-setup-for-modeling","page":"My First Model","title":"Getting your environment setup for modeling","text":"","category":"section"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"In order to build your first model, you will need to get a few more packages installed:","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"Distributions.jl : To build a simulation, you need distributions as inputs. Julia offers univariate and multivariate distributions covering most needs.\nStatsBase.jl and Statistics.jl : These packages provide all the functions to analyze results and build models.","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"To load the support packages:","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"  julia> using Distributions, Statistics, StatsBase, DataFrames","category":"page"},{"location":"tutorials/1_first_model/#Building-a-simple-example","page":"My First Model","title":"Building a simple example","text":"","category":"section"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"EVERY MONTE CARLO MODEL HAS 3 COMPONENTS","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"Inputs: Ranges or Single Values\nA Model:  Set of mathematical relationships f(x)\nOutputs: The variable(s) of interest you want to analyze","category":"page"},{"location":"tutorials/1_first_model/#Main-Distributions-for-most-modeling-situations","page":"My First Model","title":"Main Distributions for most modeling situations","text":"","category":"section"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"Though the most used distributions are cite below, Julia's Distributions package has an impressive array of options. Please check out the complete library of distributions at Distributions.jl","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"Normal()\nLogNormal()\nTriangular()\nUniform()\nBeta()\nExponential()\nGamma()\nWeibull()\nPoisson()\nBinomial()\nBernoulli()","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"In order to define a simulated input you need to use the rand function. By assigning a variable name, you can generate any simulated vector you want.","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"using Distributions, Random\nRandom.seed!(1)\ninput_variable = rand(Normal(0,1),100)","category":"page"},{"location":"tutorials/1_first_model/#Creating-a-simple-model","page":"My First Model","title":"Creating a simple model","text":"","category":"section"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"A model is either a visual or mathematical representation of a situation or system. The easiest example of a model is","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"PROFIT = REVENUE - EXPENSES","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"Let's create a simple simulation model with 1000 trials with the following inputs:","category":"page"},{"location":"tutorials/1_first_model/#Setup-environment-and-inputs","page":"My First Model","title":"Setup environment and inputs","text":"","category":"section"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"\nusing Distributions, StatsBase, Statistics, DataFrames, MCHammer \nn_trials = 1000\nRevenue = rand(TriangularDist(2500000,4000000,3000000), n_trials)\nExpenses = rand(TriangularDist(1400000,3000000,2000000), n_trials)\n","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"using Distributions, StatsBase, DataFrames, MCHammer #hide\nn_trials = 1000\nRevenue = rand(TriangularDist(2500000,4000000,3000000), n_trials)\nExpenses = rand(TriangularDist(1400000,3000000,2000000), n_trials)","category":"page"},{"location":"tutorials/1_first_model/#Define-a-Model-and-Outputs","page":"My First Model","title":"Define a Model and Outputs","text":"","category":"section"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"# The Model\nProfit = Revenue - Expenses\n\n#Trial Results : the Profit vector (OUTPUT)\nProfit","category":"page"},{"location":"tutorials/1_first_model/#Analyzing-the-results-in-Julia","page":"My First Model","title":"Analyzing the results in Julia","text":"","category":"section"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"# `fractiles()` allows you to get the percentiles at various increments.\n\nfractiles(Profit)","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"\ndensity_chrt(Profit)","category":"page"},{"location":"tutorials/1_first_model/#Sensitivity-Analysis","page":"My First Model","title":"Sensitivity Analysis","text":"","category":"section"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"First we need to create a sensitivity table with hcat() using both the input and output vectors.","category":"page"},{"location":"tutorials/1_first_model/","page":"My First Model","title":"My First Model","text":"\n#Construct the sensitivity input table by consolidating all the relevant inputs and outputs.\n\ns_table = DataFrame(Profit = Profit, Revenue = Revenue, Expenses = Expenses)\n\n#To produce a sensitivity tornado chart, we need to select the output against\n#which the inputs are measured for effect.\n\nsensitivity_chrt(s_table, 1, 3)","category":"page"},{"location":"manual/2_charts/#Charting-Functions","page":"Charting & Analyzing","title":"Charting Functions","text":"","category":"section"},{"location":"manual/2_charts/#Overview","page":"Charting & Analyzing","title":"Overview","text":"","category":"section"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"MCHammer offers the most important charts for building and analyzing Monte-Carlo Results. MCH_Charts contains standard simulation charts for sensitivity, density, trends (time series with confidence bands) for simulation arrays, vectors, and DataFrames.","category":"page"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"using Distributions, MCHammer, Random, StatsPlots, Dates\ntheme(:ggplot2)","category":"page"},{"location":"manual/2_charts/#Functions","page":"Charting & Analyzing","title":"Functions","text":"","category":"section"},{"location":"manual/2_charts/#Density-Chart","page":"Charting & Analyzing","title":"Density Chart","text":"","category":"section"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"density_chrt","category":"page"},{"location":"manual/2_charts/#MCHammer.density_chrt","page":"Charting & Analyzing","title":"MCHammer.density_chrt","text":"density_chrt(Data, x_label=\"Sim. Values\")\n\nData is your array (simulated or historical). x_label (optional) customizes the X-axis label.\n\n\n\n\n\n","category":"function"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"\ndist = rand(Normal(), 1000)\ndensity_chrt(dist, \"The Standard Normal\")","category":"page"},{"location":"manual/2_charts/#Histogram-Chart","page":"Charting & Analyzing","title":"Histogram Chart","text":"","category":"section"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"histogram_chrt","category":"page"},{"location":"manual/2_charts/#MCHammer.histogram_chrt","page":"Charting & Analyzing","title":"MCHammer.histogram_chrt","text":"histogram_chrt(Data, x_label=\"Sim. Values\")\n\nData is your array (simulated or historical). x_label (optional) customizes the X-axis label.\n\n\n\n\n\n","category":"function"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"histogram_chrt(dist, \"The Standard Normal\")","category":"page"},{"location":"manual/2_charts/#S-Curve","page":"Charting & Analyzing","title":"S-Curve","text":"","category":"section"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"s_curve","category":"page"},{"location":"manual/2_charts/#MCHammer.s_curve","page":"Charting & Analyzing","title":"MCHammer.s_curve","text":"s_curve(results, x_label=\"Sim. Values\"; rev=false)\n\ns_curve allows the visualization of a data set in cumulative form.\n\nYou can create a reverse emprical CDF py setting rev=true\nx_label= set the units label for the x axis.\n\n\n\n\n\n","category":"function"},{"location":"manual/2_charts/#Regular-Cumulative-Density-Function","page":"Charting & Analyzing","title":"Regular Cumulative Density Function","text":"","category":"section"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"s_curve(dist)","category":"page"},{"location":"manual/2_charts/#Reverse-Cumulative-Density-Function","page":"Charting & Analyzing","title":"Reverse Cumulative Density Function","text":"","category":"section"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"s_curve(dist; rev=true)","category":"page"},{"location":"manual/2_charts/#Sensitivity-Chart-(Tornado-Chart)","page":"Charting & Analyzing","title":"Sensitivity Chart (Tornado Chart)","text":"","category":"section"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"Analyzing variables most influential using a tornado sensitivity chart. Here is an example using the simple 3-2-1 profit model.","category":"page"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"sensitivity_chrt","category":"page"},{"location":"manual/2_charts/#MCHammer.sensitivity_chrt","page":"Charting & Analyzing","title":"MCHammer.sensitivity_chrt","text":"sensitivity_chrt(ArrayName::DataFrame, TargetCol, Chrt_Type=1)\n\nGenerate a horizontal bar chart summarizing sensitivity metrics for variables in a DataFrame, with special utility for analyzing Monte Carlo trials and exploratory data analysis.\n\nDescription\n\nThis function computes pairwise correlations between one or more target columns and every column in the input DataFrame using both Spearman (rank) and Pearson correlation measures. In addition, it calculates a measure of contribution to variance based on the squared Spearman correlations. The function then builds a horizontal bar chart where:\n\nWhen Chrt_Type == 1, the x-axis shows Spearman (rank) correlations.\nWhen Chrt_Type == 2, the x-axis displays Pearson correlations.\nFor any other value of Chrt_Type, the x-axis presents the percentage contribution to variance.\n\nEach variable is color-coded (red for negative correlations, blue for positive), providing an immediate visual cue about the direction of impact. The resulting chart is particularly helpful for assessing which inputs have the most significant effect on simulation outcomes, enabling users to prioritize variables for further analysis or model refinement.\n\nArguments\n\nArrayName::DataFrame: A DataFrame containing your simulation or experimental data, where each column represents a different variable.\nTargetCol: One or more column identifiers (symbols or strings) in ArrayName for which the sensitivity is to be measured. The function computes correlations between these target columns and all other columns.\nChrt_Type: An optional integer flag (default is 1) to choose the metric for the x-axis:\n1 — Spearman correlation (Rank Correlation)\n2 — Pearson correlation\nAny other value — Percentage contribution to variance (with sign)\n\n\n\n\n\n","category":"function"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"using DataFrames, Distributions, Random #hide\n\n# Set trials\nn_trials = 1000\n\n# Set inputs\nRevenue = rand(TriangularDist(2_500_000, 4_000_000, 3_000_000), n_trials)\nExpenses = rand(TriangularDist(1_400_000, 3_000_000, 2_000_000), n_trials)\n\n# Model\nProfit = Revenue - Expenses\n\n# Capture results\nTrials_df = DataFrame(Profit = Revenue - Expenses, Revenue = Revenue, Expenses = Expenses)\n\n# Chart sensitivity of profit (column 1 in DataFrame)\nsensitivity_chrt(Trials_df, 1)","category":"page"},{"location":"manual/2_charts/#Trend-Charts","page":"Charting & Analyzing","title":"Trend Charts","text":"","category":"section"},{"location":"manual/2_charts/#Probabilistic-Line-Chart-and-Time-Series-Charts","page":"Charting & Analyzing","title":"Probabilistic Line Chart and Time Series Charts","text":"","category":"section"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"trend_chrt","category":"page"},{"location":"manual/2_charts/#MCHammer.trend_chrt","page":"Charting & Analyzing","title":"MCHammer.trend_chrt","text":"trend_chrt(SimTimeArray, PeriodRange::Vector{Date}; x_label=\"periods\", quantiles=[0.05,0.5,0.95])\n\nVisualizes a simulated time series with confidence bands. PeriodRange must be a vector of Dates (e.g., using Dates: collect(Date(2019,1,1):Year(1):Date(2023,1,1))).\n\n\n\n\n\ntrend_chrt(SimTimeArray; x_label=\"periods\", quantiles=[0.05,0.5,0.95])\n\nVisualizes a simulated time series with confidence bands. The x-axis is a simple period index.\n\n\n\n\n\n","category":"function"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"ts_trials = []\n\n# Setup a TimeSeries simulation with MCHammer over 12 periods\nfor i in 1:1000\n    Monthly_Sales = GBMM(100_000, 0.05, 0.05, 12)\n    Monthly_Expenses = GBMM(50_000, 0.03, 0.02, 12)\n    MonthlyCOGS = Monthly_Sales .* 0.3\n    MonthlyProfit = Monthly_Sales - Monthly_Expenses - MonthlyCOGS\n    push!(ts_trials, MonthlyProfit)\nend\n\ntrend_chrt(ts_trials, x_label=\"last 12 months\")","category":"page"},{"location":"manual/2_charts/#Using-Dates-for-X-axis","page":"Charting & Analyzing","title":"Using Dates for X-axis","text":"","category":"section"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"To replace periods with a series of dates:","category":"page"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"dr = collect(Date(2019,1,1):Dates.Month(1):Date(2019,12,1))\ntrend_chrt(ts_trials, dr, x_label=\"last 12 months\")","category":"page"},{"location":"manual/2_charts/#Sources-and-References","page":"Charting & Analyzing","title":"Sources & References","text":"","category":"section"},{"location":"manual/2_charts/","page":"Charting & Analyzing","title":"Charting & Analyzing","text":"Eric Torkia, Decision Superhero Vol. 2, chapter 3 : Superpower: Modeling the Behaviors of Inputs, Technics Publishing, 2025 Available on Amazon : https://a.co/d/4YlJFzY . Volumes 2 and 3 to be released in Spring and Fall 2025.","category":"page"},{"location":"manual/distribution_fitting/#Distribution-Fitting","page":"Distribution Fitting","title":"Distribution Fitting","text":"","category":"section"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"Distribution fitting is the process of selecting and parameterizing a probability distribution that best describes observed data. Analysts accomplish this by comparing empirical data against theoretical distributions, adjusting parameters to minimize discrepancies. Methods often include statistical tests (e.g., Kolmogorov-Smirnov, Anderson-Darling), visual assessments (e.g., histograms, Q-Q plots), and numerical criteria (e.g., AIC, BIC, or log-likelihood).","category":"page"},{"location":"manual/distribution_fitting/#Fitting-Correctly","page":"Distribution Fitting","title":"Fitting Correctly","text":"","category":"section"},{"location":"manual/distribution_fitting/#Practical-Applications-for-Analysts","page":"Distribution Fitting","title":"Practical Applications for Analysts","text":"","category":"section"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"Distribution fitting is foundational for analysts across various fields:","category":"page"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"Risk Management & Finance:   Analysts fit distributions to asset returns or operational loss data to estimate Value at Risk (VaR), conduct stress testing, and manage financial risks.\nReliability Engineering:   By fitting failure-time data (such as machine lifetimes) to distributions like Weibull or Lognormal, analysts predict product lifespan and develop appropriate maintenance schedules.\nQuality Control:   Analysts use fitted distributions to monitor manufacturing processes, identifying deviations from expected behaviors to maintain quality standards.\nEnvironmental Modeling:   Distribution fitting helps predict extreme weather events, flood frequency, and environmental risks by modeling historical weather data or natural phenomena.\nDecision Analysis & Simulation:   Analysts model uncertainty in decision-making contexts by fitting distributions to historical data, enabling realistic simulations (e.g., Monte Carlo simulations) for strategic planning.","category":"page"},{"location":"manual/distribution_fitting/#Caveats-of-Fitting-Distributions-Empirically","page":"Distribution Fitting","title":"Caveats of Fitting Distributions Empirically","text":"","category":"section"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"Despite its usefulness, empirical distribution fitting has important limitations:","category":"page"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"Data Quantity and Quality:   Small or noisy datasets may lead to unstable parameter estimates, resulting in unreliable conclusions.\nOutliers and Extreme Values:   Extreme observations can significantly influence the fitted distribution parameters, potentially distorting insights unless handled appropriately.\nOverfitting Risk:   Analysts might select overly complex distributions that closely fit historical data but generalize poorly to new observations, limiting predictive power.\nMisinterpretation of Statistical Tests:   Statistical tests like Kolmogorov-Smirnov or Anderson-Darling can indicate a good fit even when practical considerations (such as data context) suggest otherwise, or vice versa.\nSubjective Judgment:   Empirical fitting often requires subjective judgment when choosing between multiple similarly good fits, potentially introducing bias or inconsistency.","category":"page"},{"location":"manual/distribution_fitting/#Importance-of-Selecting-Appropriate-Distributions-Beforehand","page":"Distribution Fitting","title":"Importance of Selecting Appropriate Distributions Beforehand","text":"","category":"section"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"Understanding appropriate theoretical distributions prior to analysis is crucial:","category":"page"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"Domain Knowledge:   Analysts familiar with the underlying process or theoretical context of the data (e.g., finance, engineering, biology) can immediately focus on distributions that are known to realistically model the phenomena.\nStatistical Properties:   Different distributions have distinct properties such as skewness, kurtosis, tail heaviness, and bounds. Analysts must select distributions that inherently align with these features of the data.\nAvoiding Mis-Specification:   Selecting inappropriate distributions can lead to misinterpretation of risk, poor resource allocation, or misguided policy decisions. Knowledgeable selection beforehand reduces the likelihood of such errors.\nInterpreting Results:   Using contextually appropriate distributions enhances the interpretability of model parameters, helping stakeholders more clearly understand risks, probabilities, and implications of the analysis.","category":"page"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"When used carefully, distribution fitting is a valuable analytical tool enabling informed decision-making under uncertainty. However, analysts must carefully consider data quality, model complexity, and theoretical context to ensure robust, meaningful insights from their fitted distributions.","category":"page"},{"location":"manual/distribution_fitting/#Functions-for-analyzing-and-fitting-distributions","page":"Distribution Fitting","title":"Functions for analyzing and fitting distributions","text":"","category":"section"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"Visualizing Fits: viz_fit\nDescriptive Fit Statistics: fit_stats\nAutomatic Distribution Fitting: autofit_dist","category":"page"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"","category":"page"},{"location":"manual/distribution_fitting/#Visually-analyzing-fits","page":"Distribution Fitting","title":"Visually analyzing fits","text":"","category":"section"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"viz_fit","category":"page"},{"location":"manual/distribution_fitting/#MCHammer.viz_fit","page":"Distribution Fitting","title":"MCHammer.viz_fit","text":"viz_fit(SampleData; DistFit=[], cumulative=false)\n\nVisualizes sample data against fitted probability density functions (PDFs) and cumulative densidty functions (CDFs).\n\nArguments\n\nSampleData: Array of sample data.\nDistFit (optional): Array of distribution types to fit.   If not provided, defaults to [Normal, LogNormal, Uniform].\n\n-cumulative (optional): Returns the results in cumulative form. Default is in probability density form.\n\nReturns\n\nA plot object with the density of SampleData overlaid by the fitted PDFs / CDFs.\n\n\n\n\n\n","category":"function"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"\n# Generate sample data from a Normal distribution\nusing MCHammer, Random #hide\nrng = MersenneTwister(1)\nRandom.seed!(42)\nsample_data = randn(1000)\nfit_result = viz_fit(sample_data)\n","category":"page"},{"location":"manual/distribution_fitting/#Fit-vs-Data-Stats","page":"Distribution Fitting","title":"Fit vs Data Stats","text":"","category":"section"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"fit_stats","category":"page"},{"location":"manual/distribution_fitting/#MCHammer.fit_stats","page":"Distribution Fitting","title":"MCHammer.fit_stats","text":"fit_stats(SampleData; DistFit=[], pvals=true, Increment=0.1)\n\nCalculates descriptive statistics for the sample data and for each fitted distribution in DistFit.\n\nArguments\n\nSampleData: Array of sample data.\nDistFit (optional): Pre-selected array of distribution types to fit.   Defaults to [Normal, LogNormal, Uniform] if not provided.\npvals (optional): Displays percentiles for the sample data and the fits\nIncrement (optional): Increment for the percentiles (e.g., 0.1 for 0%, 10%, …, 100%).   Default is 0.1.\n\nReturns\n\nA DataFrame (transposed) containing descriptive statistics (mean, median, mode, std, variance, skewness, kurtosis, etc.)  for the sample data and for each fitted distribution.\n\nWhen pvals = true percentiles are added the to stats table\n\nPercentile: The percentile (in %).\nSampleData: The corresponding quantile of the sample data.\nOne column per fitted distribution containing the theoretical quantiles.\n\n\n\n\n\n","category":"function"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"# Generate sample data from a LogNormal distribution\nusing Distributions, MCHammer, Random #hide\nRandom.seed!(42)\nsample_data = rand(LogNormal(0, 1), 1000)\nfits = fit_stats(sample_data)\nshow(fits, allrows=true, allcols=true)\n","category":"page"},{"location":"manual/distribution_fitting/#Automatic-Fitting","page":"Distribution Fitting","title":"Automatic Fitting","text":"","category":"section"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"MCHammer.autofit_dist","category":"page"},{"location":"manual/distribution_fitting/#MCHammer.autofit_dist","page":"Distribution Fitting","title":"MCHammer.autofit_dist","text":"autofit_dist(SampleData; DistFit=nothing, FitLib=nothing, sort=\"AIC\", verbose=false)\n\nFits a list of candidate distributions to the provided sample data, computes goodness-of-fit statistics, and returns a DataFrame summarizing the results.\n\nArguments\n\nSampleData: Array of sample data.\nDistFit (optional): Array of distribution types to attempt (e.g., [Normal, Gamma, Exponential]). Each element must be a type that is a subtype of Distribution. If provided, this overrides FitLib.\nFitLib (optional): Symbol indicating a predefined library of distributions to use. Valid options include :all, :continuous, or :discrete. Defaults to :continuous if neither DistFit nor FitLib is provided.\nsort (optional): String indicating the criterion to sort the results. Options include \"ad\", \"ks\", \"ll\", or \"AIC\". Defaults to \"AIC\".\nverbose (optional): Boolean flag indicating whether to print warnings for distributions that fail to fit. Defaults to false.\n\nReturns\n\nA DataFrame with the following columns:\n\nDistName: The name of the distribution type.\nADTest: Anderson-Darling test statistic.\nKSTest: Kolmogorov-Smirnov test statistic.\nAIC: Akaike Information Criterion.\nAICc: Corrected AIC.\nLogLikelihood: Log-likelihood of the fit.\nFitParams: The parameters of the fitted distribution.\n\n\n\n\n\n","category":"function"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"using Distributions, MCHammer, Random # hide\nRandom.seed!(42)\nsample_data = rand(LogNormal(0, 1), 1000)\nfits = autofit_dist(sample_data)\nshow(fits, allrows=true, allcols=true)\n","category":"page"},{"location":"manual/distribution_fitting/#Sources-and-References","page":"Distribution Fitting","title":"Sources & References","text":"","category":"section"},{"location":"manual/distribution_fitting/","page":"Distribution Fitting","title":"Distribution Fitting","text":"Eric Torkia, Decision Superhero Vol. 2, chapter 3 : Superpower: Modeling the Behaviors of Inputs, Technics Publishing, 2025\nAvailable on Amazon : https://a.co/d/4YlJFzY . Volumes 2 and 3 to be released in Spring and Fall 2025.","category":"page"},{"location":"manual/4_moving_results/#Importing-and-Exporting-Simulation-Results","page":"Import / Export Data","title":"Importing and Exporting Simulation Results","text":"","category":"section"},{"location":"manual/4_moving_results/#Overview","page":"Import / Export Data","title":"Overview","text":"","category":"section"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"Simulation operates using the simple constructor :Data In , Data Out or Inputs -> Model -> Outputs.  This section covers importing inputs, perhaps from a different department or external source and then exporting the results for consumption by the interested stakeholders.","category":"page"},{"location":"manual/4_moving_results/#Getting-Started","page":"Import / Export Data","title":"Getting Started","text":"","category":"section"},{"location":"manual/4_moving_results/#How-are-simulation-results-organized?","page":"Import / Export Data","title":"How are simulation results organized?","text":"","category":"section"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"Essentially all Monte-Carlo simulation results take the form of a 2 dimensional table consisting of Trials (rows) and Variables (columns). Each variable is a vector (column) of numbers in a specific order. The order is important because it is what maintains the correlation structure. For example, because of the ordered sequence, you could calculate the R2 or correlation using any 2 output vector or input vectors.","category":"page"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"Another thing to keep in mind is that each trial is assigned an ID and the generated by pushing a set of inputs into a model and recording the results in a row. Below is an example of a simulation results for Revenue - Expenses = Profit [output].","category":"page"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"Trial_ID Revenue Expenses Profit\n1 $1000 $500 $500\n2 $400 $700 $-300\n3 $700 $500 $200\n... ... ... ...","category":"page"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"A simulation results table should contain inputs variables and output variables side by side. This is a good practice because in theory it allows you to reconstruct/validate the results. In all cases, the number of trials/rows, must be of consistent length for all inputs and outputs in the model. (i.e. A 1000 trials). Each trial should have a trial ID. You can easily generate a Trial ID vector using TrialID = collect(1:1000)_ and append the column to your results using hcat().","category":"page"},{"location":"manual/4_moving_results/#Using-the-SIPMath-standard-to-organize-simulation-results","page":"Import / Export Data","title":"Using the SIPMath standard to organize simulation results","text":"","category":"section"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"SIPMath and probability management are approaches that were popularized by Sam Savage in 2009. A SIP Is known technically as a stochastic information packet or vector. The basic idea behind SIPMath is that models can be simplified and broken out into submodels. Generally, the output of a submodel can be summarized as an output vector and saves a lot of the computing efforts required to rerun both models together side-by-side.","category":"page"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"In the SIPmath Standard, uncertainties are communicated as data arrays called SIPs (Stochastic Information Packets). For example, the SIP representing the roll of a die would be expressed as thousands of outcomes, which could be stored in Excel or a database. The open SIPmath™ Standard enables legacy and future simulation models to communicate with each other, ushering in a new paradigm for enterprise risk management.","category":"page"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"[siplogo]","category":"page"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"The SIPMath standard can be consulted on the probabilitymanagement.org website","category":"page"},{"location":"manual/4_moving_results/#Importing-Variables-/-Data.","page":"Import / Export Data","title":"Importing Variables / Data.","text":"","category":"section"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"Importing results from another model for use in your analysis is a core element in building shared/collaborative models. The goal is to leverage the expertise/existing models/knowledge residing in other parts of the organization into a new model.","category":"page"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"In Julia, You can pretty much important thing you like with formats such as XML, JSON, CSV. Furthermore, most Internet data APIs will allow you to download either of these formats and gives you the ability to have an auto updating model. Though you can use anything available in Julia to import your data into a data frame, MC Hammer handles CSV the most efficiently. There are 2 basic ways to import data into your MCHammer monte-carlo model:","category":"page"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"using standard CSV tables. Julia's CSV package is quite an effective tool and converts a CSV table into a data frame. As mentioned, as long as you remember that the number of trials must be consistent, you can import almost any table from the Internet.\nusing the SIPMath 2.0 CSV Standard. MCHammer provides several functions to do so.","category":"page"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"To import from the SIPModeler tool (also available for free from ProbabilityManagement.org), we recommend using the importxlsip()","category":"page"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"importxlsip","category":"page"},{"location":"manual/4_moving_results/#MCHammer.importxlsip","page":"Import / Export Data","title":"MCHammer.importxlsip","text":"importxlsip(FileName, source=\"\")\n\nThis function allows to import SIPs in CSV format from Excel using the SIP 2.0 Standard (Stochastic Information Packets, Savage[2009]) to build unified simulation models. Based on the CSV package, the function cleans up meta data and cleans out redundant columns. You can also set a source string that will be included in your SIP DataFrame as a dimension.\n\n\n\n\n\n","category":"function"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"When importing data using the strict 2.0 standard from another environment, including R, Python or some other business/planning system, we recommend using the importsip() function instead.","category":"page"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"importsip","category":"page"},{"location":"manual/4_moving_results/#MCHammer.importsip","page":"Import / Export Data","title":"MCHammer.importsip","text":"importsip(FileName, source=\"Not specified by user\")\n\nThis function allows to import SIPs in CSV format from Julia using the SIP 2.0 Standard (Stochastic Information Packets, Savage[2009]) to build unified simulation models. Based on the CSV package, the function cleans up meta data and cleans out redundant columns. You can also set a source string that will be included in your SIP DataFrame as a dimension.\n\n\n\n\n\n","category":"function"},{"location":"manual/4_moving_results/#Exporting-results","page":"Import / Export Data","title":"Exporting results","text":"","category":"section"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"Exporting results from a simulation built in Julia can also be done using either the standard CSV package or with the functions included in MC Hammer to export using the SIPMath standard. One of the main differences when working in a programmatic environment versus a spreadsheet is that the metadata needs to reside in a separate file first before it can be recombined in the final export format. To do so, we have created both a function for incorporating MetaData into your SIP library and one for exporting the simulation results.","category":"page"},{"location":"manual/4_moving_results/#Generating-MetaData","page":"Import / Export Data","title":"Generating MetaData","text":"","category":"section"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"genmeta","category":"page"},{"location":"manual/4_moving_results/#MCHammer.genmeta","page":"Import / Export Data","title":"MCHammer.genmeta","text":"genmeta(source_df, s_name=\"SLURPName\")\n\nSIPs using the SIP 2.0 Standard (Stochastic Information Packets, Savage[2009]) require that Meta Data be available and maintained. This function creates a file template to add MetaData to your SIP Library. This is a seperate file must accompany the DataFrame to generate the SIP Library correctly. If ommited, the file will export the SIP Library without any metadata.\n\n\n\n\n\n","category":"function"},{"location":"manual/4_moving_results/#Exporting-Results-in-the-CSV-SIPMath-2.0-standard.","page":"Import / Export Data","title":"Exporting Results in the CSV SIPMath 2.0 standard.","text":"","category":"section"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"sip2csv","category":"page"},{"location":"manual/4_moving_results/#MCHammer.sip2csv","page":"Import / Export Data","title":"MCHammer.sip2csv","text":"sip2csv(FileName, source_df, s_name=\"SLURP\", s_origin=\"Julia Language\")\n\nIn order to export a SIP Library from Julia, you simply need to have a DataFrame.\nOnly specify header fields if your data does not contain any.\nAlso make to specify the full filename, including extension\n\n\n\n\n\n","category":"function"},{"location":"manual/4_moving_results/","page":"Import / Export Data","title":"Import / Export Data","text":"[siplogo]: https://github.com/etorkia/MCHammer.jl/tree/master/docs/src/assets/siplogo.png","category":"page"},{"location":"tutorials/2_Correlated_Model/#Correlating-Variables-in-Your-Model","page":"Correlating Inputs","title":"Correlating Variables in Your Model","text":"","category":"section"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"To get started, we are going to recap what we did in your first model and build a correlated version of the same model and compare.","category":"page"},{"location":"tutorials/2_Correlated_Model/#Building-a-Simple-Uncorrelated-Model","page":"Correlating Inputs","title":"Building a Simple Uncorrelated Model","text":"","category":"section"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"using DataFrames, MCHammer, Plots, Distributions,Statistics, StatsBase, StatsPlots, Dates, TimeSeries, Random\n\nn_trials = 10000\nRandom.seed!(1)\nRevenue = rand(TriangularDist(2500000,4000000,3000000), n_trials)\nExpenses = rand(TriangularDist(1400000,3000000,2000000), n_trials)\nInput_Table = DataFrame(Revenue=Revenue, Expenses=Expenses)","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"using Distributions, StatsBase, DataFrames, MCHammer\nn_trials = 10000\nRandom.seed!(1)\nRevenue = rand(TriangularDist(2500000,4000000,3000000), n_trials)\nExpenses = rand(TriangularDist(1400000,3000000,2000000), n_trials)\n\n# The Model\nProfit = Revenue - Expenses","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"#Trial Results : the Profit vector (OUTPUT)\nProfit","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"# Trials or Results Table (OUTPUT)\nTrials = DataFrame(Revenue = Revenue, Expenses = Expenses, Profit = Profit)\nfirst(Trials,20)","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"# Uncorrelated simulation\ncormat(Trials)","category":"page"},{"location":"tutorials/2_Correlated_Model/#Applying-correlation-to-your-simulation","page":"Correlating Inputs","title":"Applying correlation to your simulation","text":"","category":"section"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"Using the corvar() function, we are going to correlate the Revenue and Expenses at -0.8 and generate the results tables for both the correlated and uncorrelated versions.","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"#Correlate the expenses and the revenue with a coefficient of 0.8\nRev_Exp_Cor = 0.8\ncor_matrix = [1 Rev_Exp_Cor; Rev_Exp_Cor 1]\n\n#Validate input correlation. You can also use cormat() to define the correlation\n#matrix from historical data.\ncor_matrix","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"It is very important to join Trial into an array before applying correlation. Furthermore, this step is necessary in order to produce a sensitivity_chrt()","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"Input_Table = DataFrame(Revenue=Revenue, Expenses=Expenses)\nCorrel_Trials = corvar(Input_Table, n_trials, cor_matrix)\nDataFrames.rename!(Correl_Trials, [:Revenue, :Expenses])\n\n#Using the correlated inputs to calculate the correlated profit\nCorrel_Trials.Profit = Correl_Trials.Revenue - Correl_Trials.Expenses","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"#Verify correlation is applied correctly\ncormat(Correl_Trials)","category":"page"},{"location":"tutorials/2_Correlated_Model/#Analyze-the-impact-of-correlation-on-your-output","page":"Correlating Inputs","title":"Analyze the impact of correlation on your output","text":"","category":"section"},{"location":"tutorials/2_Correlated_Model/#Input-Correlation-between-Revenue-and-Expenses","page":"Correlating Inputs","title":"Input Correlation between Revenue and Expenses","text":"","category":"section"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"Input Correlation:","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"cor(Revenue,Expenses)","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"Input Correlation for the Correlated Model:","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"cor(Correl_Trials.Revenue, Correl_Trials.Expenses)","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"You can query with the charting and stats functions using these Model Outputs: Trials, Correl_Trials, Profit, Correl_Trials.Profit","category":"page"},{"location":"tutorials/2_Correlated_Model/#Correlated-vs.-Uncorrelated-results-in-Julia","page":"Correlating Inputs","title":"Correlated vs. Uncorrelated results in Julia","text":"","category":"section"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"Let us compare the percentiles of an uncorrelated  model vs. a correlated one.","category":"page"},{"location":"tutorials/2_Correlated_Model/#Probability-Analysis","page":"Correlating Inputs","title":"Probability Analysis","text":"","category":"section"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"\ncompresults_df = DataFrame(uprofit = Profit, cprofit = Correl_Trials.Profit )\n\n@df stack(compresults_df) density(:value, group=:variable, legend=:topright, title=\"Density Plot\")","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"Using GetCertainty() we can do some simple probability accounting to assess the likelyhood of making 1m or less in profit :","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"GetCertainty(Profit, 1000000, 0)","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"GetCertainty(Correl_Trials.Profit, 1000000, 0)","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"By accounting for the correlation, we can see the probability of achieving our profit objective dropped by about 5%","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"fractiles() allows you to get the percentiles at various increments to be able to compare results along a continuum.","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"#Uncorrelated\nfractiles(Profit)","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"#Correlated\nfractiles(Correl_Trials.Profit)","category":"page"},{"location":"tutorials/2_Correlated_Model/#Comparing-the-impact-of-the-inputs","page":"Correlating Inputs","title":"Comparing the impact of the inputs","text":"","category":"section"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"Sensitivity of uncorrelated results:","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"sensitivity_chrt(Trials,3)","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"Sensitivity of correlated results","category":"page"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"sensitivity_chrt(Correl_Trials,3)","category":"page"},{"location":"tutorials/2_Correlated_Model/#A-quick-analysis-of-the-results","page":"Correlating Inputs","title":"A quick analysis of the results","text":"","category":"section"},{"location":"tutorials/2_Correlated_Model/","page":"Correlating Inputs","title":"Correlating Inputs","text":"Accounting for correlation meant a 5% (~42% vs. ~47%) reduction in probability of not making our goals.\nThe Worse Case goes from -290k to 230k, a 225% difference\nThe critical driver in both cases is expenses.","category":"page"},{"location":"manual/ExponentialSmoothing/#Forecasting-with-Exponential-Smoothing","page":"Exponential Smoothing","title":"Forecasting with Exponential Smoothing","text":"","category":"section"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"This page documents the exponential smoothing methods and functions provided by the package, including detailed explanations, docstrings, and usage examples.","category":"page"},{"location":"manual/ExponentialSmoothing/#Method-Functions","page":"Exponential Smoothing","title":"Method Functions","text":"","category":"section"},{"location":"manual/ExponentialSmoothing/#Simple-Exponential-Smoothing","page":"Exponential Smoothing","title":"Simple Exponential Smoothing","text":"","category":"section"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Simple exponential smoothing model. Fields:","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"alpha::Float64: Smoothing constant.","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"using MCHammer, DataFrames, Distributions, Random, Plots, StatsPlots, GraphRecipes #hide","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Example:","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"using MCHammer #hide\nmodel = MCHammer.SimpleES(0.2)","category":"page"},{"location":"manual/ExponentialSmoothing/#Double-Exponential-Smoothing","page":"Exponential Smoothing","title":"Double Exponential Smoothing","text":"","category":"section"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Double exponential smoothing model with trend. Fields:","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"alpha::Float64: Level smoothing constant.\nbeta::Float64: Trend smoothing factor.","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Example:","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"using MCHammer #hide\nmodel = DoubleES(0.3, 0.1)","category":"page"},{"location":"manual/ExponentialSmoothing/#Triple-Exponential-Smoothing","page":"Exponential Smoothing","title":"Triple Exponential Smoothing","text":"","category":"section"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Triple exponential smoothing model (Holt–Winters method) with trend and seasonality.","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Fields:","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"season_length::Int: Number of periods in a season (e.g., 12 for monthly).\nalpha::Float64: Level smoothing constant.\nbeta::Float64: Trend smoothing factor.\ngamma::Float64: Seasonal smoothing factor.","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Example:","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"using MCHammer #hide\nmodel = TripleES(12, 0.3, 0.2, 0.1)","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"","category":"page"},{"location":"manual/ExponentialSmoothing/#Smoothing-Functions","page":"Exponential Smoothing","title":"Smoothing Functions","text":"","category":"section"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Perform smoothing on historical data.","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"MCHammer.es_smooth ","category":"page"},{"location":"manual/ExponentialSmoothing/#MCHammer.es_smooth","page":"Exponential Smoothing","title":"MCHammer.es_smooth","text":"Perform simple exponential smoothing on the historical series.\n\nes_smooth(m::SimpleES, HistoricalSeries::Vector{<:Real}; forecast_only::Bool=false)\n\nArguments\n\nm::SimpleES: A SimpleES model containing the smoothing constant alpha.\nHistoricalSeries: A vector of historical data.\nforecast_only (optional): If true, only the smoothed forecast is returned.\n\nReturns\n\nA dataframe of smoothed values or the original and smoothed values.\n\n\n\n\n\nPerform double exponential smoothing on the historical series.\n\nes_smooth(m::DoubleES, HistoricalSeries::Vector{<:Real})\n\nArguments\n\nm::DoubleES: A DoubleES model with alpha and beta.\nHistoricalSeries: A vector of historical data.\n\nReturns\n\nA DataFrame where:\n\nlevel: The smoothed level values.\ntrend: The estimated trend values.\nsmoothed: The sum of level and trend.\n\n\n\n\n\n","category":"function"},{"location":"manual/ExponentialSmoothing/#Smoothing-SimpleES:","page":"Exponential Smoothing","title":"Smoothing SimpleES:","text":"","category":"section"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"using DataFrames, Distributions, Random, MCHammer #hide\n\ndata = [100, 102, 104, 108, 110]\nsimple_model = SimpleES(0.2)\nsmoothed_simple = es_smooth(simple_model, data; forecast_only=true)","category":"page"},{"location":"manual/ExponentialSmoothing/#Smoothing-DoubleES:","page":"Exponential Smoothing","title":"Smoothing DoubleES:","text":"","category":"section"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"# DoubleES example\nusing MCHammer, DataFrames, Distributions, Random # hide\ndouble_model = DoubleES(0.2, 0.1)\nsmoothed_double = es_smooth(double_model, data)","category":"page"},{"location":"manual/ExponentialSmoothing/#Forecasting-out","page":"Exponential Smoothing","title":"Forecasting out","text":"","category":"section"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Generate forecasts based on smoothing models.","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"MCHammer.es_forecast","category":"page"},{"location":"manual/ExponentialSmoothing/#MCHammer.es_forecast","page":"Exponential Smoothing","title":"MCHammer.es_forecast","text":"Produce a forecast using double exponential smoothing.\n\nes_forecast(m::DoubleES, HistoricalSeries::Vector{<:Real}, periods::Int)\n\nArguments\n\nm::DoubleES: A DoubleES model with alpha and beta.\nHistoricalSeries: A vector of historical data.\nperiods: Number of periods to forecast beyond the historical data.\n\nReturns\n\nA DataFrame with columns: Historical, Level, and Forecast.\n\n\n\n\n\nProduce a forecast using triple exponential smoothing (Holt–Winters method) with seasonal adjustment.\n\nes_forecast(m::TripleES, HistoricalSeries::Vector{<:Real}, periods_out::Int; forecast_only::Bool=false)\n\nArguments\n\nm::TripleES: A TripleES model with season_length, alpha, beta, and gamma.\nHistoricalSeries: A vector of historical data.\nperiods_out: Number of periods to forecast beyond the historical data.\nforecast_only (optional): If true, returns only the forecasted values.\n\nReturns\n\nEither a vector of forecasted values (if forecast_only=true) or the complete in-sample forecast.\n\n\n\n\n\n","category":"function"},{"location":"manual/ExponentialSmoothing/#Forecasting-DoubleES:","page":"Exponential Smoothing","title":"Forecasting DoubleES:","text":"","category":"section"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"using MCHammer,Plots # hide\n\ndata = [100, 102, 104, 108, 110]\n\n# DoubleES forecast\ndouble_model = DoubleES(0.2, 0.1)\ndf_forecast = es_forecast(double_model, data, 3)","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"<br> Let's visualize this as a line plot comparing historical data and the forecast <br>","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"using Plots\n\n# Create an index vector corresponding to the rows.\nx_all = 1:nrow(df_forecast)\n\n# Identify the indices where Historical is nonzero.\nnonzero_idx = findall(x -> x != 0, df_forecast.Historical)\n\n# Plot forecast over all rows.\nplot(x_all, df_forecast.Forecast, label=\"Forecast\", xlabel=\"Time\", ylabel=\"Value\", lw=2)\n\n# Plot Historical and Level only on nonzero indices.\nplot!(nonzero_idx, df_forecast.Historical[nonzero_idx], label=\"Historical\", marker=:circle, lw=2)\nplot!(nonzero_idx, df_forecast.Level[nonzero_idx], label=\"Level\", marker=:square, lw=2)\n","category":"page"},{"location":"manual/ExponentialSmoothing/#Forecasting-TripleES:","page":"Exponential Smoothing","title":"Forecasting TripleES:","text":"","category":"section"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"# TripleES forecast\nusing MCHammer, DataFrames, Distributions, Random # hide\n\nseasonal_data = [120,130,140,130,125,135,145,150,160,155,165,170]\ntriple_model = MCHammer.TripleES(12, 0.3, 0.2, 0.1)\ndf_forecast = es_forecast(triple_model, seasonal_data, 6; forecast_only=false)","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Let's visualize this as a line plot comparing historical and forecasted data","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"using Plots\ntheme(:ggplot2)\n\n# Create an index vector corresponding to the rows.\nx_all = 1:nrow(df_forecast)\n\n# Identify the indices where Historical is nonzero.\nnonzero_idx = findall(x -> x != 0, df_forecast.Historical)\n\n# Plot forecast over all rows.\nplot(x_all, df_forecast.Forecast, label=\"Forecast\", xlabel=\"Time\", ylabel=\"Value\", lw=2)\n\n# Plot Historical and Level only on nonzero indices.\nplot!(nonzero_idx, df_forecast.Historical[nonzero_idx], label=\"Historical\", marker=:circle, lw=2)\n","category":"page"},{"location":"manual/ExponentialSmoothing/#Forecast-Standard-Error","page":"Exponential Smoothing","title":"Forecast Standard Error","text":"","category":"section"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Calculate standard error between historical and forecasted data.","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"MCHammer.FrctStdError","category":"page"},{"location":"manual/ExponentialSmoothing/#MCHammer.FrctStdError","page":"Exponential Smoothing","title":"MCHammer.FrctStdError","text":"Calculate the fractional standard error between the historical series and forecast series.\n\nFrctStdError(HistoricalSeries::Vector{<:Real}, ForecastSeries::Vector{<:Real})\n\nArguments\n\nHistoricalSeries: A vector of historical data.\nForecastSeries: A vector of forecasted data.\n\nReturns\n\nThe standard error as a Float64.\n\n\n\n\n\n","category":"function"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Example:","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"using MCHammer # hide\n\ndata = [100, 102, 104, 108, 110]\ndouble_model = DoubleES(0.3, 0.2)\n\n# Generate in-sample forecast; since periods = 0, the forecast equals the smoothed level\n\ndf_forecast = es_forecast(double_model, data, 0)\nforecasted = df_forecast.Forecast  # Access the forecast column from the returned DataFrame\nse = FrctStdError(data, forecasted)\nprintln(\"Forecast Standard Error: \", se)","category":"page"},{"location":"manual/ExponentialSmoothing/#Fitting-historical-data","page":"Exponential Smoothing","title":"Fitting historical data","text":"","category":"section"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Automatically optimize parameters for TripleES.","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"MCHammer.es_fit","category":"page"},{"location":"manual/ExponentialSmoothing/#MCHammer.es_fit","page":"Exponential Smoothing","title":"MCHammer.es_fit","text":"Automatically fit optimal parameters for triple exponential smoothing by random trials.\n\nes_fit(::Type{TripleES}, HistoricalSeries::Vector{<:Real}, season_length::Int, trials::Int)\n\nArguments\n\nHistoricalSeries: A vector of historical data.\nseason_length: Number of periods in a season.\ntrials: Number of random trials to perform.\n\nReturns\n\nA TripleES instance with the best parameters (lowest forecast error).\n\n\n\n\n\n","category":"function"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"using MCHammer # hide\ndata = [120,130,140,130,125,135,145,150,160,155,165,170]\nbest_model = es_fit(TripleES, data, 12, 1000)","category":"page"},{"location":"manual/ExponentialSmoothing/#Simulating-forecasts-(Monte-Carlo)","page":"Exponential Smoothing","title":"Simulating forecasts (Monte-Carlo)","text":"","category":"section"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Simulate forecast uncertainty using historical returns.","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"forecast_uncertainty","category":"page"},{"location":"manual/ExponentialSmoothing/#MCHammer.forecast_uncertainty","page":"Exponential Smoothing","title":"MCHammer.forecast_uncertainty","text":"forecast_uncertainty(HistoricalData::Vector{<:Real}, PeriodsToForecast::Int) -> Vector{Float64}\n\nCalculate forecast uncertainty multipliers based on historical data volatility.\n\nThis function computes the percentage returns from the historical data, estimates the standard deviation (σ) of those returns, and then generates a vector of forecast multipliers. Each multiplier is calculated as:\n\nu = 1 + sigma cdot epsilon\n\nwhere (epsilon) is a random sample drawn from a standard normal distribution          epsilon sim mathcalN(01) and sigma is the standard deviation of the historical percentage returns.\n\nArguments\n\nHistoricalData::Vector{<:Real}: A vector of historical observations (e.g., prices, values).\nPeriodsToForecast::Int: The number of forecast periods for which to generate uncertainty multipliers.\n\nReturns\n\nA vector of length PeriodsToForecast containing the forecast uncertainty multipliers. These multipliers can be used to perturb a base forecast to simulate forecast variability.\n\n\n\n\n\n","category":"function"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Practical Example:","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"using MCHammer, DataFrames, Distributions, Random # hide\n\n#The historical data is used to assess the volatility of the series automatically.\ndata = [100, 105, 102, 108, 110]\n\nuncertainty = forecast_uncertainty(data, 4)","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Now let's assume we want to run 1000 Monte-Carlo Trials, this is the approach we would take.","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"using MCHammer, DataFrames, Distributions, Random # hide\n\nfunction simulate_ESTS()\n\n#Simulation Model Inputs and Parameters\nn_trials = 1000\nn_periods = 5\nhistorical_data = [100, 105, 102, 108, 110, 115, 120, 118, 122, 125, 130, 128]\nseasonality = 12\n\n    # 1. Fit optimal TripleES parameters using 1,000 optimization trials. Seasonality is the number of periods to test for seasonal patterns.\n    optimal_triple = es_fit(TripleES, historical_data, seasonality, 1000)\n\n    # 2. Generate the base forecast (for n_periods = 5) with the fitted model\n    base_forecast = es_forecast(optimal_triple, historical_data, n_periods; forecast_only=true)\n\n    # 3. Run simulation: for 1,000 trials, apply forecast uncertainty to the base forecast\n\n\n    # Prepare a DataFrame to store simulation results in long format.\n    # Columns: Trial (simulation number), Period (forecast period), Forecast (simulated forecast value)\n    sim_results = DataFrame(Trial=Int[], Period=Int[], Forecast=Float64[])\n\n        for trial in 1:n_trials\n        # Compute forecast uncertainty multipliers for the next n_periods\n        uncertainty = forecast_uncertainty(historical_data, n_periods)\n        \n        # Apply the uncertainty multipliers to the base forecast to simulate forecast variability.\n        # This returns a DataFrame with columns \"Historical\", \"Level\", and \"Forecast\".\n        simulated_forecast = base_forecast .* uncertainty\n\n        # Now iterate over the rows using eachrow() and enumerate to get an index.\n            for (i, row) in enumerate(eachrow(simulated_forecast))\n                push!(sim_results, (Trial = trial, Period = i, Forecast = row.Forecast))\n            end\n            \n        end\n\n\n        return sim_results = unstack(sim_results, :Period, :Forecast)\n\nend\n\n#Genreate simulation trials\nsim_results_to_chart = simulate_ESTS()\n\n#Here are the first 10 trials\nfirst(sim_results_to_chart,10)\n","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"When plotted, we can see the uncertainty is applied using historical volatility. Remember to remove the trials column for better charting.","category":"page"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"trend_chrt(sim_results_to_chart[:,2:6])","category":"page"},{"location":"manual/ExponentialSmoothing/#Sources-and-References","page":"Exponential Smoothing","title":"Sources & References","text":"","category":"section"},{"location":"manual/ExponentialSmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Eric Torkia, Decision Superhero Vol. 3, chapter 5 : Predicting 1000 futures, Technics Publishing, 2025\nAvailable on Amazon : https://a.co/d/4YlJFzY . Volumes 2 and 3 to be released in Spring and Fall 2025.","category":"page"},{"location":"manual/3_time_series/#Random-Probability-Time-Series","page":"Random & Probability Methods","title":"Random + Probability Time-Series","text":"","category":"section"},{"location":"manual/3_time_series/#Overview","page":"Random & Probability Methods","title":"Overview","text":"","category":"section"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"MCH Timeseries contains functions to create simulated times series with MCHammer. Current implementation supports Geometric Brownian Motion, Martingales and Markov Chain Time Series. ","category":"page"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"DocTestSetup = quote\n    rng = MersenneTwister(1)\n    Random.seed!(1)\n    BrandShare = [0.1, 0.25, 0.05, 0.35, 0.25]\n\n    DrinkPreferences =\n    [0.6\t0.03 0.15 0.2 0.02;\n    0.02 0.4 0.3 0.2 0.08;\n    0.15\t0.25\t0.3 0.25\t0.05;\n    0.15\t0.02\t0.1\t0.7\t0.03;\n    0.15\t0.3 0.05\t0.05\t0.45]\n\nend","category":"page"},{"location":"manual/3_time_series/#Random-Walks-(Geometric-Brownian-Motion)","page":"Random & Probability Methods","title":"Random Walks (Geometric Brownian Motion)","text":"","category":"section"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"Geometric Brownian Motion is commonly used for simulating financial time series, such as stock prices. It models continuous price paths where changes follow a log-normal distribution.","category":"page"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"GBMMfit","category":"page"},{"location":"manual/3_time_series/#MCHammer.GBMMfit","page":"Random & Probability Methods","title":"MCHammer.GBMMfit","text":"GBMMfit(HistoricalData, PeriodsToForecast; rng=\"none\")\n\nGBMMfit uses a vector of historical data to calculate the log returns and use the mean and standard deviation to project a random walk. It the uses the last datapoint in the set as the starting point for the new forecast.\n\nHistoricalData: Vector containing historical data\n\nPeriodsToForecast: integer >1\n\nrng is fully random when set to none. You can specify the rng you want to seed the results e.g. MersenneTwister(1234)\n\n\n\n\n\n","category":"function"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"using MCHammer, Random, Distributions\nrng = MersenneTwister(1)\nRandom.seed!(1)\nhistorical = rand(Normal(10,2.5),1000)\n\nGBMMfit(historical, 12; rng=rng)\n\n# output\n12×1 Matrix{Float64}:\n 7.3005613535018785\n 9.941620113944857\n 7.103000072680243\n 3.5244881063798483\n 0.5845383089109127\n 0.8153566312249856\n 1.072169905405429\n 1.0457817586351268\n 1.1470926126750904\n 0.8738792037186909\n 1.0021288627898237\n 1.7966621213142604","category":"page"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"GBMM","category":"page"},{"location":"manual/3_time_series/#MCHammer.GBMM","page":"Random & Probability Methods","title":"MCHammer.GBMM","text":"GBMM(LastValue, ReturnsMean, ReturnsStd, PeriodsToForecast; rng::Any=\"none\")\n\nGBMM produces a random walk using the last data point and requires a mean and standard deviation to be provided.\n\nLastValue: The most recent data point on which to base your random walk.\n\nReturnsMean and ReturnsStd : Historical Mean and Standard Deviation of Returns\n\nPeriodsToForecast is an integer >1\n\nrng is fully random when set to none. You can specify the rng you want to seed the results e.g. MersenneTwister(1234)\n\n\n\n\n\n","category":"function"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"using MCHammer, Random, Distributions\nRandom.seed!(1)\nrng = MersenneTwister(1)\nGBMM(100000, 0.05,0.05,12, rng=rng)\n\n# output\n12×1 Matrix{Float64}:\n 104315.42290790279\n 114538.65544311896\n 115918.4421778525\n 113954.88460165854\n 107025.58832570235\n 117985.02450091281\n 128814.81609339731\n 134829.80326014754\n 143300.85485171276\n 145928.17297856198\n 156063.17236356856\n 180291.78429354102","category":"page"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"GBMA_d","category":"page"},{"location":"manual/3_time_series/#MCHammer.GBMA_d","page":"Random & Probability Methods","title":"MCHammer.GBMA_d","text":"GBMA_d(price_0, t, rf, exp_vol; rng=\"none\")\n\nGBMA_d allows you to forecast the stock price at a given day in the future. This function uses a multiplicative Geometric Brownian Motion to forecast but using the Black-Scholes approach\n\n**price_0**: Stock Price at period 0\n**t**: Number of days out to forecast\n**rf**: Risk free rate (usually the country's 25 or 30 bond)\n**exp_vol**: Expected volatility is the annual volatility\n**rng**: Used seed the results to make the forecast reproducible. Set to none by default which means fully random  but can use any of Julia's rngs to seed results. e.g. *GBMA_d(price_0, t, rf, exp_vol; rng=MersenneTwister(1))*\n\n\n\n\n\n","category":"function"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"using MCHammer, Random #hide\nrng = MersenneTwister(1)\nRandom.seed!(1)\nGBMA_d(100, 504,0.03,.3, rng=rng)\n\n# output\n88.86175908928719","category":"page"},{"location":"manual/3_time_series/#Simulating-a-random-walk-time-series","page":"Random & Probability Methods","title":"Simulating a random walk time-series","text":"","category":"section"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"using Dates, MCHammer, Random, Distributions\nRandom.seed!(1)\n\nts_trials =[]\n\n#To setup a TimeSeries simulation with MCHammer\nfor i = 1:1000\n     Monthly_Sales = GBMM(100000, 0.05,0.05,12)\n     Monthly_Expenses = GBMM(50000, 0.03,0.02,12)\n     MonthlyCOGS = Monthly_Sales .* 0.3\n     MonthlyProfit = Monthly_Sales - Monthly_Expenses - MonthlyCOGS\n     push!(ts_trials, MonthlyProfit)\nend\n\n#You can graph the result using trend_chrt()\ndr = collect(Date(2025,1,01):Dates.Month(1):Date(2025,12,31))\ntrend_chrt(ts_trials,dr)","category":"page"},{"location":"manual/3_time_series/#Martingales","page":"Random & Probability Methods","title":"Martingales","text":"","category":"section"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"A stochastic time-series modeled as a martingale describes a process where each subsequent value's expected future outcome is equal to the current observed value, conditional on the history of all past values. It characterizes a fair, unbiased random walk without drift, commonly applied in scenarios like fair gambling games, financial markets under risk-neutral conditions, or unbiased forecasting models.","category":"page"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"using Dates, MCHammer, Random, Plots, Distributions, DataFrames\ntheme(:ggplot2)","category":"page"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"marty","category":"page"},{"location":"manual/3_time_series/#MCHammer.marty","page":"Random & Probability Methods","title":"MCHammer.marty","text":"marty(Wager, GamesPlayed; GameWinProb = 0.5, CashInHand = Wager)\n\nIn probability theory, a martingale is a sequence of random variables (i.e., a stochastic process) for which, at a particular time, the conditional expectation of the next value in the sequence, given all prior values, is equal to the present value. (Wikipedia)\n\nThe marty function is designed to simulate a Martigale such as that everytime a wager is lost, the next bet doubles the wagered amount to negate the previous loss. The resulting vector is the balance of cash the gambler has in hand at any given point in the Martingale process.\n\nGameWinProb is the estimated probability of winning. CashInHand is the starting balance for the martigale. At times, this parameter can make a difference in whether your survive the process or go home broke.\n\n\n\n\n\n","category":"function"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"For example a gambler with 50$ making wagers of 50$, 10 times using the double or nothing strategy.","category":"page"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"marty(50,10)","category":"page"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"Now let's assume that the gambler knows the odds of winning  at the casino are less than 0.5 and decides to bring additional funds to persist until the bet pays off.","category":"page"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"println(marty(50,10; GameWinProb=0.45, CashInHand=400))","category":"page"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"Using Plots.jl,  you can compare outcomes at different win probabilities","category":"page"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"using Plots\n\n# Generate the four plots as scatter plots.\nExp11 = scatter(marty(5, 100, GameWinProb = 0.25, CashInHand = 400), title=\"GameWinProb = 0.25\", label=\"Bets\", markerstrokecolor=:white, markercolor=:lightblue, titlefontsize=10)\n\nExp12 = scatter(marty(5, 100, GameWinProb = 0.33, CashInHand = 400), title=\"GameWinProb = 0.33\", label=\"Bets\", markerstrokecolor=:white, markercolor=:lightblue, titlefontsize=10)\n\nExp13 = scatter(marty(5, 100, GameWinProb = 0.5,  CashInHand = 400), title=\"GameWinProb = 0.5\", label=\"Bets\", markerstrokecolor=:white, markercolor=:lightblue, titlefontsize=10)\n\nExp14 = scatter(marty(5, 100, GameWinProb = 0.55, CashInHand = 400), title=\"GameWinProb = 0.55\", label=\"Bets\", markerstrokecolor=:white, markercolor=:lightblue, titlefontsize=10)\n\n#) Combine them into a 2x2 grid layout.\ncombined = Plots.plot(Exp11, Exp12, Exp13, Exp14, layout=(2,2), legend=:topleft)\n","category":"page"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"","category":"page"},{"location":"manual/3_time_series/#Sources-and-References","page":"Random & Probability Methods","title":"Sources & References","text":"","category":"section"},{"location":"manual/3_time_series/","page":"Random & Probability Methods","title":"Random & Probability Methods","text":"Eric Torkia, Decision Superhero Vol. 2, chapter 7 : SuperPower – Modeling Probability and Random Events, 2025\nEric Torkia, Decision Superhero Vol. 3, chapter 5 : Predicting 1000 futures, Technics Publishing, 2025","category":"page"},{"location":"manual/1_functions/#Simulation-Modeling-Functions","page":"Monte-Carlo Simulation","title":"Simulation Modeling Functions","text":"","category":"section"},{"location":"manual/1_functions/#Overview","page":"Monte-Carlo Simulation","title":"Overview","text":"","category":"section"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"Though most of your modeling can be done directly in raw Julia, some of the most important features in a Monte-Carlo simulation package involve analyzing and applying correlation in models. MCHammer's correlation approach is based on:","category":"page"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"Ronald L. Iman & W. J. Conover (1982) A distribution-free approach to inducing rank correlation among input variables, Communications in Statistics - Simulation and Computation","category":"page"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"The simulation and correlation functions are designed to quickly obtain risk and decision analysis metrics, such as moments, percentiles, and risk over time.","category":"page"},{"location":"manual/1_functions/#Risk-Events-and-Conditional-Distributions","page":"Monte-Carlo Simulation","title":"Risk Events and Conditional Distributions","text":"","category":"section"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"Risk Events allow you to model a joint distribution accounting for the probability of an event occurring and the conditional impact when it does. The process simulates the Probability x Impact formula correctly.","category":"page"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"RiskEvent","category":"page"},{"location":"manual/1_functions/#MCHammer.RiskEvent","page":"Monte-Carlo Simulation","title":"MCHammer.RiskEvent","text":"RiskEvent(Prob, Distribution, Trials; seed=0)\n\nRisk Events are defined as conditional distributions that will inflate the 0.\n\nThe RiskEvent() allows you to conditionally sample any distribution for as many trials as defined. Prob is the conditional probability of sampling the impact Distribution Distribution is any univariate distribution from Distributions.jl Trials is the number of total iterations. seed allows you to set a seed for the RiskEvent. Left blank or set to 0, the seed is set to random.\n\n\n\n\n\n","category":"function"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"using MCHammer, Distributions, Random #hide\n# Simulate a conditional risk event with a 30% chance of occurring and an impact\n# that is distributed according to a standard Normal. 10 trials are generated and\n# about 3 should have non-zero outcomes.\n\nusing MCHammer #hide\n\nRiskEvent(0.3, Normal(0,1), 10)","category":"page"},{"location":"manual/1_functions/#Correlation-and-Covariance","page":"Monte-Carlo Simulation","title":"Correlation and Covariance","text":"","category":"section"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"Correlation and covariance functions help you understand relationships between different simulation inputs. In practical modeling:","category":"page"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"Use Cases: Assess which variables move together (positive/negative correlation) to reduce or amplify overall risk.\nApplications: Finance (portfolio risk), engineering (linked component failures), supply chain planning (demand and lead-time co-fluctuation).","category":"page"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"cormat","category":"page"},{"location":"manual/1_functions/#MCHammer.cormat","page":"Monte-Carlo Simulation","title":"MCHammer.cormat","text":"  cormat(ArrayName, RankOrder=1)\n\nCormat calculates a symetric correlation matrix using both PPMC and Rank Order. Rank Order is default because this is what it used in the Iman-Conover method for correlating of simulated variables.\n\nRankOrder = 1 calculates the Spearman rank order correlation used in MCHammer (this argument is optional and defaults to Spearman)\n\nRankOrder = 0 calculates the Pearson Product Moment Correlation\n\n\n\n\n\n","category":"function"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"using Distributions, MCHammer, Random #hide\nrng = MersenneTwister(1)\nRandom.seed!(1)\ntest = rand(Normal(), 1000, 5)\ncormat(test)\n\n# output\n\n5×5 Matrix{Float64}:\n  1.0         0.0262401  -0.0119314    0.0386272   -0.0755551\n  0.0262401   1.0        -0.0118889    0.0137545   -0.0305986\n -0.0119314  -0.0118889   1.0         -0.00762943  -0.0264234\n  0.0386272   0.0137545  -0.00762943   1.0         -0.00137442\n -0.0755551  -0.0305986  -0.0264234   -0.00137442   1.0","category":"page"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"covmat","category":"page"},{"location":"manual/1_functions/#MCHammer.covmat","page":"Monte-Carlo Simulation","title":"MCHammer.covmat","text":"  covmat(ArrayName)\n\nCalculates the covariance matrix.\n\n\n\n\n\n","category":"function"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"using Distributions, MCHammer, Random #hide\nrng = MersenneTwister(1)\nRandom.seed!(1)\ntest = rand(Normal(), 1000, 5)\ncovmat(test)\n\n# output\n5×5 Matrix{Any}:\n  1.01123      0.0293154    0.00167811   0.0401224   -0.0820211\n  0.0293154    1.11648     -0.00357405   0.0142628   -0.032553\n  0.00167811  -0.00357405   1.05398     -0.0033127   -0.0285162\n  0.0401224    0.0142628   -0.0033127    0.872718    -0.00241684\n -0.0820211   -0.032553    -0.0285162   -0.00241684   1.01745\n","category":"page"},{"location":"manual/1_functions/#Correlating-Simulation-Variables","page":"Monte-Carlo Simulation","title":"Correlating Simulation Variables","text":"","category":"section"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"corvar adjusts your simulated datasets to match a desired correlation matrix. In modeling:","category":"page"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"Use Cases: Forces scenario data to reflect real-world or hypothesized correlations, ensuring valid joint distributions.\nApplications: Stress tests in finance, correlated risk analysis (e.g., hurricane + flood), and multivariate forecasting.","category":"page"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"corvar","category":"page"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"using Distributions, MCHammer, Random #hide\nrng = MersenneTwister(1)\nRandom.seed!(1)\nn_trials = 1000\nsample_data = [rand(rng, LogNormal(0, 0.5), n_trials) rand(rng, Normal(3,2), n_trials) rand(rng, Gamma(1, 0.5), n_trials) rand(rng, LogNormal(0, 0.5), n_trials) rand(rng, Normal(3,2), n_trials) rand(rng, Gamma(1, 0.5), n_trials)]\n\ntest_cmatrix = [1 0 0 0 0 0; 0 1 0 0 0 0; 0 0 1 0 0 0;0 0 0 1 0.75 -0.7; 0 0 0 0.75 1 -0.95; 0 0 0 -0.7 -0.95 1 ]\n\ncormat(corvar(sample_data, n_trials, test_cmatrix))\n\n# output\n\n6×6 Matrix{Float64}:\n  1.0         0.0262401  -0.0119314    0.0386272   -0.0295832   0.034389\n  0.0262401   1.0        -0.0118889    0.0137545   -0.0160956   0.0147439\n -0.0119314  -0.0118889   1.0         -0.00762943  -0.0242712   0.0420867\n  0.0386272   0.0137545  -0.00762943   1.0          0.713931   -0.659495\n -0.0295832  -0.0160956  -0.0242712    0.713931     1.0        -0.939242\n  0.034389    0.0147439   0.0420867   -0.659495    -0.939242    1.0","category":"page"},{"location":"manual/1_functions/#Analyzing-Simulation-Results","page":"Monte-Carlo Simulation","title":"Analyzing Simulation Results","text":"","category":"section"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"Result analysis functions like GetCertainty and fractiles help interpret simulation outputs:","category":"page"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"Use Cases: Identify probability of crossing thresholds (e.g., \"chance of negative profit\"), or measure percentile-based risk.\nApplications: Cost-risk analysis, reliability engineering, or performance metrics (min, max, median) in uncertain environments.","category":"page"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"GetCertainty","category":"page"},{"location":"manual/1_functions/#MCHammer.GetCertainty","page":"Monte-Carlo Simulation","title":"MCHammer.GetCertainty","text":"  GetCertainty(ArrayName, x, AboveBelow=0)\n\nThis function returns the percentage of trials Above (1) or Below(0) a target value of x.\n\n\n\n\n\n","category":"function"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"using Random, MCHammer, Distributions #hide\nrng = MersenneTwister(1)\nRandom.seed!(1)\ntest = rand(Normal(), 1000)\n\nGetCertainty(test, 0, 1)\n\n# output\n\n0.528","category":"page"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"fractiles","category":"page"},{"location":"manual/1_functions/#MCHammer.fractiles","page":"Monte-Carlo Simulation","title":"MCHammer.fractiles","text":"  fractiles(ArrayName, Increment=0.1)\n\nThe fractiles function calculates percentiles at equal increments. The default optional argument for Increments is 0.1 for deciles but can be set to anything such as 0.05 for quintiles or 0.01 for percentiles.\n\n\n\n\n\n","category":"function"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"using Random, MCHammer, Distributions #hide\nrng = MersenneTwister(1)\nRandom.seed!(1)\ntest = rand(Normal(), 1000)\n\nfractiles(test)\n\n# output\n\n11×2 Matrix{Any}:\n \"P0.0\"    -2.95049\n \"P10.0\"   -1.30285\n \"P20.0\"   -0.834923\n \"P30.0\"   -0.472307\n \"P40.0\"   -0.227086\n \"P50.0\"    0.0882202\n \"P60.0\"    0.319301\n \"P70.0\"    0.579717\n \"P80.0\"    0.873708\n \"P90.0\"    1.29457\n \"P100.0\"   2.97612","category":"page"},{"location":"manual/1_functions/#Misc-Functions","page":"Monte-Carlo Simulation","title":"Misc Functions","text":"","category":"section"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"These utility functions provide convenience in data cleanup and quick calculation:","category":"page"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"Use Cases: Scripting and reporting automation where formatting or command line interactions are needed.\nApplications: Rounding values for dashboards, truncating decimal places in sensitivity outputs, or running shell commands mid-simulation.","category":"page"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"cmd","category":"page"},{"location":"manual/1_functions/#MCHammer.cmd","page":"Monte-Carlo Simulation","title":"MCHammer.cmd","text":"  cmd(x)\n\nShell /Dos Command wrapper to run batch and shell commands in script. This is used to process SQL from the command line or perform system level operation in a script using a command prompt.\n\n\n\n\n\n","category":"function"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"truncate_digit","category":"page"},{"location":"manual/1_functions/#MCHammer.truncate_digit","page":"Monte-Carlo Simulation","title":"MCHammer.truncate_digit","text":"function  truncate_digit(num, digits=2)\n\nTruncation algorithim to remove decimals (ported by anonymous author from Maple) e.g.\n\n  0.066 = 0.06\n  0.063 = 0.06\n\n\n\n\n\n","category":"function"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"using Random, MCHammer #hide\nResult_1 = truncate_digit(0.667)\nResult_2 = truncate_digit(0.661)\nResult_1 == Result_2\n\n# output\n\ntrue","category":"page"},{"location":"manual/1_functions/#Sources-and-References","page":"Monte-Carlo Simulation","title":"Sources & References","text":"","category":"section"},{"location":"manual/1_functions/","page":"Monte-Carlo Simulation","title":"Monte-Carlo Simulation","text":"Eric Torkia, Decision Superhero Vol. 2, chapter 5 : Predicting 1000 futures, Technics Publishing 2025\nAvailable on Amazon : https://a.co/d/4YlJFzY . Volumes 2 and 3 to be released in Spring and Fall 2025.","category":"page"},{"location":"#MCHammer.jl","page":"Home","title":"MCHammer.jl","text":"","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The MC in MC Hammer stands for Monte-Carlo. This tool is inspired by seminal tools such as Oracle Crystal Ball and Palisade @RISK for their ability to quickly build and analyze Monte-Carlo simulation models using Excel functions and automations. MC Hammer replicates their logic, functions and elemental tools in Julia, thus significantly reducing the time, lines of code, complexity and effort to perform advanced modeling and simulation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Most of the code and functions were developped in the Decision Superhero series by Eric Torkia (Available on Amazon : https://a.co/d/4YlJFzY). Volumes 2 and 3 to be released in Spring and Fall 2025. MCHammer was partly developped as the companion software to volumes 2 and 3. ","category":"page"},{"location":"#Key-Features:","page":"Home","title":"Key Features:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Comprehensive Simulation Functions: MCHammer.jl offers a suite of tools and user-friendly wrapper functions for risk events, correlation and covariance analysis, and simulation result evaluation, allowing you to gain deeper insights into your models. \nAdvanced Charting and Analysis: Visualize your simulation results with built-in functions for density plots, histograms, sensitivity analyses, and trend charts, facilitating a clearer understanding of data patterns and relationships. \nTime-Series Simulation: Generate simulated time series using methods like Geometric Brownian Motion and Martingales, making it ideal for modeling financial data and other temporal datasets. \nData Import/Export: Seamlessly import and export simulation results, ensuring smooth integration with other data analysis workflows. ","category":"page"},{"location":"#Get-Started:","page":"Home","title":"Get Started:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Begin your journey with MCHammer.jl by following the comprehensive tutorials available on the website. Learn to build your first model, correlate inputs, and simulate cash flow scenarios with step-by-step guidance.","category":"page"},{"location":"#Contribute-to-the-Project:","page":"Home","title":"Contribute to the Project:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MCHammer.jl thrives on community collaboration. Whether you're interested in reporting issues, suggesting enhancements, or contributing code, your involvement is highly valued. Visit the GitHub repository to join the community and help drive the project forward. Embrace the power of Monte Carlo simulations in Julia with MCHammer.jl. Download it today and be part of a growing community dedicated to advancing simulation modeling. ","category":"page"},{"location":"#Installing-MCHammer","page":"Home","title":"Installing MCHammer","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Install the package as usual using Pkg.","category":"page"},{"location":"","page":"Home","title":"Home","text":"    using Pkg\n    Pkg.(\"MCHammer\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you need to install direct, we recommend using ']' to go in the native Pkg manager.","category":"page"},{"location":"","page":"Home","title":"Home","text":"    (v1.11) pkg> add https://github.com/etorkia/MCHammer.jl","category":"page"},{"location":"#Loading-MCHammer","page":"Home","title":"Loading MCHammer","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To load the MCHammer package","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MCHammer","category":"page"},{"location":"#Getting-your-environment-setup-for-modeling","page":"Home","title":"Getting your environment setup for modeling","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In order to build your first model, you will need to get a few more packages installed:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Distributions.jl : To build a simulation, you need distributions as inputs. Julia offers univariate and multivariate distributions covering most needs.\nStatsBase.jl and Statistics.jl : These packages provide all the functions to analyze results and build models.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To load the support packages:","category":"page"},{"location":"","page":"Home","title":"Home","text":"  julia> using Distributions, Statistics, StatsBase, DataFrames","category":"page"},{"location":"#Tutorials","page":"Home","title":"Tutorials","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Building your first model : This simple exercise shows you how to build quick simulation models using MCHammer.\nCorrelating Variables in Your Model: Similar to the first tutorial, here we show users how to correlate variables together in their simulations. \nSimulated NPV with Time-Series: Basic skeleton to build discounted cashflows with simulation and charting, like Oracle Crystal Ball or '@RISK' ","category":"page"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"tutorials/3_NPV_testmodel/#Simulated-NPV-with-Time-Series","page":"Simulated CashFlow Model","title":"Simulated NPV with Time-Series","text":"","category":"section"},{"location":"tutorials/3_NPV_testmodel/#Load-Environment","page":"Simulated CashFlow Model","title":"Load Environment","text":"","category":"section"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"Let's start by making sure all the tools we nned are loaded up. You will almost always need to load these packages up anytime you are build a Monte-Carlo model.","category":"page"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"using Distributions\nusing Dates\nusing Plots, StatsPlots\nusing StatsBase, Statistics\nusing MCHammer\nusing DataFrames","category":"page"},{"location":"tutorials/3_NPV_testmodel/#Setup-Inputs-and-Outputs","page":"Simulated CashFlow Model","title":"Setup Inputs and Outputs","text":"","category":"section"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"The next critical step is to setup key inputs, arrays and other important model parameters.","category":"page"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"#Setup the Date Range for the analysis\ndr = collect(Date(2019,1,01):Dates.Year(1):Date(2023,01,01))\n\n#Setup Global Inputs\nForecastYrs = 5\nTrials = 10000\nUnits = [5000, 10000, 17000, 18000, 20000]\nInitialInvestment = 250000\nInvestment = [100000, 0, 0, 25000,0] #fill(0,ForecastYrs)\n\n#Setup Outputs\nSensitivity_Tbl = []\n\nProjectNPV = []\nUSP = []\nUSC =[]\nDR = []\nOP =[]\nAnnual_CashFlows =[]","category":"page"},{"location":"tutorials/3_NPV_testmodel/#Simulation-Model","page":"Simulated CashFlow Model","title":"Simulation Model","text":"","category":"section"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"Monte-Carlo simulation needs to generate a table of scenarios which are known as Trials. A trial documents, in the form of a row, all of the inputs and calculated outputs for a particular scenario.","category":"page"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"Using this results table allows you to runs all sorts of analysis, including sensitivity analysis and assigning probabilities to outcomes. To generate this table, you need to loop your equation/function as many times as you need and vary the inputs using probability distributions.","category":"page"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"Another challenge to account for is that our example is a 5yr NPV model which requires building and analyzing the results over multiple periods. To extend the model, we are using MCHammer's GBMM function that allows to project a random walk forecast over how ever many periods you need, which extends automatically the model in Julia.","category":"page"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"for i = 1:Trials\n    UnitSellPrice = GBMM(80, 0.2, 0.1, ForecastYrs)\n    UnitCost = GBMM(40, 0.1, 0.05, ForecastYrs)\n\n    #Each period the discount rate is independent. If you use an additive method instead of multiplicative, you can end up with differences. These may or may not impact the decision. For simulation it is best to use the risk free rate.\n\n        #Multiplicative Method\n        DiscountRate = cumprod(rand(Normal(0.02,0.0075),ForecastYrs)+fill(1,ForecastYrs))#accumulate(+,rand(Normal(0.02,0.0075),ForecastYrs))+fill(1,ForecastYrs)\n\n        #Additive Method\n        #DiscountRate = accumulate(+,rand(Normal(0.02,0.0075),ForecastYrs))+fill(1,ForecastYrs)\n\n        #a static DR\n        #DiscountRate = accumulate(+, fill(0.02,ForecastYrs))+fill(1,ForecastYrs)\n\n    #print(DiscountRate)\n\n    #DCF Elements\n    Annual_Sales = UnitSellPrice .* Units\n    Annual_COGS = UnitCost .* Units\n    OPEX = rand(TriangularDist(.2,0.5,0.35),ForecastYrs) .* Annual_Sales\n\n    #Constant Dollar Cashflow\n    #CashFlow_C = (Annual_Sales - Annual_COGS - OPEX - Investment)\n\n    #Discounted CashFLow over multpile periods. This function uses arrays and DOT functions.\n\n    CashFlow = (Annual_Sales - Annual_COGS - OPEX - Investment) ./ DiscountRate\n\n    #Calculated Output\n    Trial_NPV = sum(CashFlow)-InitialInvestment\n\n#Convert Arrays to Scalars for sensitivity analysis\n    push!(ProjectNPV, Trial_NPV)\n    push!(USC, mean(UnitCost))\n    push!(USP, mean(UnitSellPrice))\n    push!(DR,  mean(DiscountRate))\n    push!(OP,  mean(OPEX))\n    push!(Annual_CashFlows,  CashFlow)\nend","category":"page"},{"location":"tutorials/3_NPV_testmodel/#Setting-up-data-for-analysis-and-charting","page":"Simulated CashFlow Model","title":"Setting up data for analysis and charting","text":"","category":"section"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"Setup inputs/outputs(above) and output tables (below) for sensitivity analysis and charting. Since correlation is based on the same math as regression, the only way to calculate sensitivity on an Array > 1 (in this case multiple years) is to condense the array into a scalar value using either mean, sum or any other transform because what ever you pick will generate a similar or identical result.","category":"page"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"Sensitivity_Tbl = DataFrame(ProjectNPV = ProjectNPV, USC = USC, USP = USP, DR = DR, OP = OP)\nsensitivity_chrt(Sensitivity_Tbl,1)","category":"page"},{"location":"tutorials/3_NPV_testmodel/#Stats","page":"Simulated CashFlow Model","title":"Stats","text":"","category":"section"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"Generate model results and list all the outputs for your charting and analysis functions.","category":"page"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"print(\"Project Mean: \", mean(ProjectNPV),\"\\n\")\nprint(\"Project Std.Dev: \", std(ProjectNPV),\"\\n\")\nprint(\"Prob. of Neg. NPV: \", GetCertainty(ProjectNPV,0,0),\"\\n\")\nprint(\"NPV p10, p50, p90 : \", quantile(collect(Float64, ProjectNPV),[0.1,0.5,0.9]),\"\\n\")\nprintln(\"\")\nprintln(\"OUTPUTS: Annual_CashFlows, ProjectNPV, Sensitivity_Tbl\")\nprintln(\"date range = dr\")","category":"page"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"To generate a complete list of percentiles, use the fractiles().","category":"page"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"fractiles(ProjectNPV)","category":"page"},{"location":"tutorials/3_NPV_testmodel/#Looking-at-the-Probability-Distribution","page":"Simulated CashFlow Model","title":"Looking at the Probability Distribution","text":"","category":"section"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"histogram_chrt(ProjectNPV, \"Five Year NPV\")","category":"page"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"s_curve(ProjectNPV, \"Five Year NPV\")","category":"page"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"density_chrt(ProjectNPV, \"Five Year NPV\")","category":"page"},{"location":"tutorials/3_NPV_testmodel/#What-variables-are-most-influential-on-my-output-distribution?","page":"Simulated CashFlow Model","title":"What variables are most influential on my output distribution?","text":"","category":"section"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"sensitivity_chrt(Sensitivity_Tbl, 1, 3)","category":"page"},{"location":"tutorials/3_NPV_testmodel/#What-does-my-cashflow-look-like-over-time?","page":"Simulated CashFlow Model","title":"What does my cashflow look like over time?","text":"","category":"section"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"The trend chart is a median centered chart that establishes a 90% confidence interval for each period. Remember dr or the date range is specified at the top.","category":"page"},{"location":"tutorials/3_NPV_testmodel/#CashFlow-forecast","page":"Simulated CashFlow Model","title":"CashFlow forecast","text":"","category":"section"},{"location":"tutorials/3_NPV_testmodel/","page":"Simulated CashFlow Model","title":"Simulated CashFlow Model","text":"trend_chrt(Annual_CashFlows, dr)","category":"page"}]
}
